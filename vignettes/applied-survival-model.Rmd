---
title: "Applied Survival Models"
author: "Jacqueline Buros Novik"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Applied survival models}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, eval = T, results = 'hide', echo = F}
knitr::opts_chunk$set(message = FALSE,
                      warning = FALSE,
                      fig.width = 11,
                      fig.height = 6
                      )
```


```{r load-packages, eval = T, echo = F}
library(httr)
library(readr)
library(cgdsr)
library(purrr)
library(dplyr)
library(assertthat)
library(ggplot2)
library(survival)
library(rstan)
rstan_options(auto_write = TRUE)
options(mc.cores = min(4, parallel::detectCores()))
library(shinystan)
library(gridExtra)
library(ggfortify)
```

Survival modeling is a core component of any clinical data analysis toolset.

Right-censoring is by far the most common scenario, since patients cannot be followed indefinitely. There is also valuable information in the distribution of times to events, even in cases where all patients experience an outcome.

Fitting survival models in Stan is fairly straightforward. In this section, we will describe the model, fit the model to simulated data, and illustrate with an applied example.

The last part of this vignette includes some suggestions of ways to extend the standard Cox Proportional Hazards model. We will get to these if time allows.

# Background

In a survival modeling context, the dependent variable of interest is typically *time to event*. 

We will start by considering a single source of failure (death) vs censored observations.

We start with a parametric model, then implement a non-parametric fit.

# Data

We will illustrate this analysis using data from [The Cancer Genoma Atlas (TCGA)](https://tcga-data.nci.nih.gov) for patients diagnosed with Bladder Urothelial Carcinoma.

The complete clinical & molecular data are available from TCGA data portals, but in this case we will use a curated version of these data available from <http://www.cbioportal.org/>.

We *could* query these data via the web-url service: 

```{r example-load-data, eval = FALSE}
url <- 'http://www.cbioportal.org/webservice.do?cmd=getClinicalData&case_set_id=blca_tcga_all'
req <- httr::GET(url)
clinical_data <- 
    httr::content(req,
                  type = 'text/tab-separated-values',
                  col_names = T,
                  col_types = NULL
                  )
str(clinical_data)
```

However, MSKCC has provided the [CGDS-R](http://cran.r-project.org/web/packages/cgdsr/index.html) package, which provides an easier interface to the same data.

```{r actual-load-data}
mycgds = cgdsr::CGDS("http://www.cbioportal.org/public-portal/")
selected_case_list = 'blca_tcga_all'
clinical_data = cgdsr::getClinicalData(mycgds, selected_case_list)
```

List of fields available:

```{r inspect-data}
str(clinical_data,  no.list = T, vec.len = 2)
```

Let's do some minimal data manipulation on these data before going any further. We will save the converted data in a data.frame called `clin_data`.

```{r initprep-data}
## names to lower case
names(clinical_data) <- tolower(names(clinical_data))

## convert empty strings -> NA values
convert_blank_to_na <- function(x) {
    if (!purrr::is_character(x)) {
        warning('input vector is not character - returning original input')
        return(x)
    } else {
        ifelse(x == '', NA, x)
    }
}
clin_data <- clinical_data %>%
    dplyr::mutate_each(funs = funs(convert_blank_to_na), everything())

## inspect resulting data frame
str(clin_data, vec.len = 2, list.len = 10)
```

# First analysis: overall survival using parametric survival model

Our first pass at this analysis will treat mortality as the primary endpoint. We will fit the data using a parametric model which assumes that failure times are distributed according to a weibull distribution.

## Data prep

The relevant data are contained within two variables: `os_status` & `os_months`. 

For various reasons, these data include one observation with missing data for Overall Survival

```{r inspect-os-status}
clinical_data %>%
    dplyr::filter(is.na(os_status) | os_status == '') %>%
    dplyr::select(os_status, os_months) %>%
    str()
```

In addition, we have a 3 observations with unknown and/or negative survival times (`os_months < 0`).

```{r inspect-os-months}
clinical_data %>%
    dplyr::filter(!is.na(os_status) & os_status != '') %>%
    dplyr::filter(os_months < 0 | is.na(os_months)) %>%
    dplyr::select(os_status, os_months) %>%
    head()
```

For now, remove these observations from our analysis dataset (`clin_data`).

```{r create-clin-data}
clin_data <- 
    clin_data %>%
    dplyr::filter(!is.na(os_status) & os_status != '') %>%
    dplyr::filter(os_months >= 0 & !is.na(os_months))

## confirm 4 fewer observations than original
assert_that(nrow(clin_data) == nrow(clinical_data) - 4)
```

## Data exploration

We will do some minimal data exploration before attempting to fit the model.

Numerical summary of event times by event type

```{r summary-os-months}
clin_data %>%
    split(.$os_status) %>%
    purrr::map(~ summary(.$os_months))
```

Graphical summary of event times by type

```{r plot-os-months}
ggplot(clin_data,
       aes(x = os_months,
           group = os_status,
           colour = os_status,
           fill = os_status
           )) + 
    geom_density(alpha = 0.5)
```

Summarize MLE estimate of the survival function for these data and plot the KM survival curve for observed data. 

These will give us a benchmark for future evaluations.

```{r mle-survfit}
mle.surv <- 
    survfit(
        Surv(os_months,os_deceased) ~ 1,
        data = clin_data %>%
            dplyr::mutate(os_deceased = os_status == 'DECEASED')
    )
plot(mle.surv)
```

# First analysis: parametric survival model

For our first analysis we will work with a parametric survival model implemented by [Peltola et al (2014)](http://ceur-ws.org/Vol-1218/bmaw2014_paper_8.pdf). 

**Aside** This is a nice paper that compares performance across a variety of regularizing priors (horseshoe, gaussian, and LASSO) for survival analysis, and discusses a practical strategy for  evaluation of potential biomarkers.

## Stan code for the model

The Stan file for this model is adapted from their accompanying [github repo](https://github.com/to-mi/stan-survival-shrinkage).

We can load it directly from git into our session

```{r get-stan-code}
stan_code_url <- 'https://raw.githubusercontent.com/to-mi/stan-survival-shrinkage/master/wei_bg.stan'
stan_code <- httr::content(httr::GET(stan_code_url), as = 'text')

print_stan_code(stan_code)
```

## Prepare data input for stan 

In order to call stan, we must prepare a data input object which will be passed to the compiled stan program. 
This input can be a `list` or an `environment` (since environments are implemented as lists). I prefer to 
work with lists directly.

The structure and type of elements in this `list` must match the components of the `data { ... }` block in our Stan code.

#### Review data block 

Let's review the data block again to see what we need to pass in.

```{r view-data-block}
print_stan_code(stan_code, section = 'data')
```

Notice how the censored & observed data points take separate input vectors. 

**observed data points**
* `Nobs`: number of observed data points 
* `yobs`: times to observed events
* `Xobs_bg`: "background" covariate values for observed data points

**censored data points**
* `Ncen`: number of censored data points 
* `ycen`: times to censored events
* `Xcen_bg`: "background" covariate values for censored data points

In addition there is the dimension `M_bg`, indicating the number of covariates.

In our case, we want to fit a NULL model (with no covariate values) but the Stan
code isn't written to accomodate the no-biomarker case. (Try it to see how this works).

For now, we will try to fit the model by filling in biomarker values of 0.

#### Write the input data list

Here we prepare the `list` object which we will pass to Stan

```{r stan-input-data}
observed_data <- clin_data %>%
    dplyr::filter(os_status == 'DECEASED')

censored_data <- clin_data %>%
    dplyr::filter(os_status != 'DECEASED')

stan_data <- list(
    Nobs = nrow(observed_data),
    Ncen = nrow(censored_data),
    M_bg = 1,
    yobs = observed_data$os_months,
    ycen = censored_data$os_months,
    Xobs_bg = array(rep_len(0, nrow(observed_data)), dim = c(nrow(observed_data), 1)),
    Xcen_bg = array(rep_len(0, nrow(censored_data)), dim = c(nrow(censored_data), 1))
)
str(stan_data)
```

## Setting initial values

This next step is optional, but may be necessary for some models.

For this example, we will set the initial parameters even though it's not strictly necessary. Sometimes I find it's useful and often not emphasized.

By default, Stan chooses a random initial value for each parameter on the unconstrained scale between -2 and 2. The default selection is on the unconstrained support so that the initial values are guaranteed to be consistent with the constrained range.

When we pass the initial values in, however, these are on the constrained scale. The main goal is to select the initial values at random but within a range that is reasonable for the model. 

### Plausible initial values 

Let's review the parameters block for this model again.

```{r}
print_stan_code(stan_code, section = 'parameters')
```

### Function to specify initial values 

Here is the function (functional) we define to provide initial values for each chain. This is based on the example code ([example.R](https://github.com/to-mi/stan-survival-shrinkage/blob/master/example.R#L88-L103)) file in the author's git repo.

```{r stan-init-values}
make_inits <- function(M_bg, M_biom) {
    function()
      list(
        tau_s_bg_raw = 0.1*abs(rnorm(1)),
        tau_bg_raw = array(abs(rnorm(M_bg)), dim = c(M_bg)),
        tau_s1_biom_raw = 0.1*abs(rnorm(1)),
        tau_s2_biom_raw = 0.1*abs(rnorm(1)),
        tau_biom_raw = array(abs(rnorm(M_biom)), dim = c(M_biom)),
        tau1_biom_raw = abs(rnorm(M_biom)),
        tau2_biom_raw = abs(rnorm(M_biom)),
        alpha_raw = 0.01*rnorm(1),
        beta_bg_raw = array(rnorm(M_bg), dim = c(M_bg)),
        beta_biom_raw = array(rnorm(M_biom), dim = c(M_biom)),
        mu = rnorm(1)
      )
}
```

This function will return a function which itself generates the initial values for each parameter in our stan code. It is a function wrapped in a function so that we can pass different values of `M_bg` & `M_biom`.

If this isn't obvious to you, I recommend you take a look at the output for different values of `M_bg` & `M_biom`.

for example:
```{r illustrate-init-values}
make_inits(1, 1)() %>%
    str()
```

## Fitting the model

We are now ready to try fitting the model to our data. 

### Test the model 

Personally, I test the code on a few iterations to confirm that my starting values are reasonable & that there are no problems executing the model code on the input data.

I find it's best to test on a handful of iterations & on a single chain. This makes debugging easier.

```{r stan-test-fit}
test_fit <- 
    rstan::stan(model_code = stan_code,
                data = stan_data,
                iter = 5,
                chains = 1,
                init = make_inits(1, 1))
```

Note that it's not surprising for us to have a fair number of implausible parameter values at this point. We are only a few iterations in. 

#### aside on input data types

Aside: for an example of how sensitive `stan()` is to dimension of inputs, try running the following exercize.

Compare the value of *a vector wrapped by `array()`*

```{r stan-alt-inits1a}
array(rnorm(0), dim = c(0))
```

with that of *a naked vector*

```{r stan-alt-inits2a}
rnorm(0)
```

These two look the same, don't they?

**However, looks can be deceiving. **

The data type stan is looking for is a `vector`: 

```{r review-stan-params}
for (l in 
     readLines(textConnection(
         gsub(stan_code,
              pattern = '.*(parameters \\{.*?\\}).*',
              replacement = '\\1'
              )
         ))
     )
    cat(l, '\n')
```

Ironically, the only way to force an R object to be a vector is to wrap it in the `array()` operator.

For example, compare the structure of *a vector wrapped in an array* 

```{r stan-alt-inits1b}
str(array(rnorm(0), dim = c(0)))
```

with the structure of *a naked vector*

```{r stan-alt-inits2b}
str(rnorm(0))
```

These are different objects in the eyes of `stan`. 

#### input data types in practice 

Notice what happens if we modify the `init` function to remove one of the `array()` wrappers listed above.

```{r alt-inits}
## alternate make-inits function for illustrative purposes
alt_make_inits <- function(M_bg, M_biom) {
    function()
      list(
        tau_s_bg_raw = 0.1*abs(rnorm(1)),
        tau_bg_raw = abs(rnorm(M_bg)),  ### removed array() wrapper
        tau_s1_biom_raw = 0.1*abs(rnorm(1)),
        tau_s2_biom_raw = 0.1*abs(rnorm(1)),
        tau_biom_raw = array(abs(rnorm(M_biom)), dim = c(M_biom)),
        tau1_biom_raw = abs(rnorm(M_biom)),
        tau2_biom_raw = abs(rnorm(M_biom)),
        alpha_raw = 0.01*rnorm(1),
        beta_bg_raw = array(rnorm(M_bg), dim = c(M_bg)),
        beta_biom_raw = array(rnorm(M_biom), dim = c(M_biom)),
        mu = rnorm(1)
      )
}
test_fit2 <- rstan::stan(model_code = stan_code, data = stan_data, iter = 10, chains = 1, init = alt_make_inits(1, 1))

```

... something to watch out for when passing data to stan.

## back to fitting model in stan

OK. With that covered, let's get back to fitting our model.

Next we try running this for more iterations and on multiple chains. 

*NB: here we reference the earlier fitted object instead of passing in the model code directly*

```{r stan-actual-fit}
wei_fit <- rstan::stan(fit = test_fit,
                       data = stan_data,
                       iter = 1000,
                       chains = 4,
                       init = make_inits(1, 1))
print(wei_fit)
```

## Checking convergence

Convergence can be assessed in various ways.

### Review Rhat values 

For a quick indication, we can look at the Rhat values (Gelman-Rubin convergence diagnostic) in the summary table above.

This is a useful gut-check, in that R-hat values far from 1 are a strong indication of a lack of convergence. Often the fix is as simple as running more iterations, or using a longer warmup sampling period.

An R-hat value close to one, however, does not have high sensitivity. It does not guarantee convergence.

### Inspect traceplots

We then also look at the traceplot of the log-posterior. I usually check this first since it gives me a sense of the overall model fit.

```{r lp-traceplot}
rstan::traceplot(wei_fit, c('lp__'), ncol = 1)
```

And, for key parameters (in this case, `alpha` & `mu`)

```{r param-traceplot}
rstan::traceplot(wei_fit, c('alpha','mu'), ncol = 1)
```

### Launch shinystan 

For more detailed model inspection we can leverage the awesome `shinystan`. 
This displays best-practice diagnostics (e.g. autocorrelation of chains) in an easy-to-use interface.

```{r launch-shinystan}
if (interactive())
    launch_shinystan(wei_fit)
```

## Posterior predictive checks

We can simulate draws from the posterior predictive distribution in R or in Stan. 

Since the biomarker values given weren't informative, we only want to inspect the baseline hazard or survival functions.

The only two parameters needed to specify these are `alpha` & `mu`.

```{r sim-post-predict}
sim_post_predict <- function(alpha, mu, Nobs, Ncen) {
    observed_data <- data.frame(os_status = rep_len('DECEASED', Nobs),
                                os_months = rweibull(n = Nobs, alpha, exp(-(mu)/alpha)),
                                stringsAsFactors = F
                                )
    
    censored_data <- data.frame(os_status = rep_len('LIVING', Ncen),
                                os_months_uncensored = rweibull(Ncen, alpha, exp(-(mu)/alpha)),
                                os_months = 
                                stringsAsFactors = F
                                )
    
    return(observed_data %>% bind_rows(censored_data))
}

## test these inputs for arbitrary values of alpha & mu
sim_post_predict(alpha = 0.9, mu = -3.76, Nobs = nrow(observed_data), Ncen = nrow(censored_data)) %>%
    head()
```

Let's plot the mean time to survival in our simulated data, using the posterior means

```{r mean-post-predict}

mean_estimated_mu <- mean(rstan::extract(wei_fit, 'mu')$mu)
mean_estimated_alpha <- mean(rstan::extract(wei_fit, 'alpha')$alpha)

## plot distribution of survival times by event type
ggplot(sim_post_predict(alpha = mean_estimated_alpha, mu = mean_estimated_mu, Nobs = nrow(observed_data), Ncen = nrow(censored_data))
       , aes(x = os_months, group = os_status, colour = os_status, fill = os_status)) +
    geom_density(alpha = 0.3)

```

How does this compare to the observed data?

```{r compare-mean-ppd}
ggplot(sim_post_predict(alpha = mean_estimated_alpha, mu = mean_estimated_mu, Nobs = nrow(observed_data), Ncen = nrow(censored_data)) %>%
           dplyr::mutate(type = 'posterior predicted values') %>%
           bind_rows(clin_data %>% dplyr::mutate(type = 'actual data'))
       , aes(x = os_months, group = os_status, colour = os_status, fill = os_status)) +
    geom_density(alpha = 0.5) +
    facet_wrap(~type, ncol = 1)
```

Plotting as a survival curve

```{r compare-mean-ppdsurv}
sim_post_data <- sim_post_predict(alpha = mean_estimated_alpha, mu = mean_estimated_mu, Nobs = nrow(observed_data), Ncen = nrow(censored_data))
simppd.surv <- survival::survfit(Surv(os_months,os_deceased) ~ 1, data = sim_post_data %>% dplyr::mutate(os_deceased = os_status == 'DECEASED'))
simplot <- autoplot(simppd.surv) + ggtitle('Simulated data')
mleplot <- autoplot(mle.surv) + ggtitle('Actual data')
grid.arrange(simplot, mleplot, ncol = 2)
```

Overlaying observed & simulated survival curves (still using posterior mean of parameters)

```{r compare-mean-ppdsurv2, fig.width=11, fig.height=6}
ggplot(data = 
           fortify(simppd.surv) %>% 
           mutate(src = 'posterior predicted values') %>% 
           bind_rows(fortify(mle.surv) %>% mutate(src = 'actual data')),
       aes(x = time, group = src)) + 
    geom_line(aes(y = surv, colour = src)) + 
    geom_ribbon(aes(ymin = lower, ymax = upper, fill = src), alpha = 0.2) +
    ggtitle('Survival curve for observed and posterior predicted event times \n (simulated according to mean of posterior parameter estimates)')
```

## Further posterior predictive checking

But, this is only an approximation to the true posterior predictive checks we want to do. The means of the parameter estimates don't reflect our true uncertainty in the parameter estimates.

To get to this point, we will want to repeat the above-described process *for each draw from the posterior*.

Let's do this now.

First let's review how to extract the parameters from the stanfit object.

```{r}
pp_mu <- extract(wei_fit, 'mu')$mu
str(pp_mu)
```

Why do we have 2000 values? 

(answer: 4 chains * 1000 iterations / 2 since by default warmup iters are dropped)

We should have a similar structure of values for `alpha`.

```{r}
pp_alpha <- extract(wei_fit, 'alpha')$alpha
str(pp_alpha)
```

To get the estimated variance in our survival curves, we will want to apply the same steps we did above (using the single point-estimate of the mean of alpha & mu) to each iteration.

Thanks to @hadley, we can do this using `purrr`.

```{r}
models <- 
    purrr::map2(pp_alpha, pp_mu, ~ sim_post_predict(alpha = .x, mu = .y, Nobs = nrow(observed_data), Ncen = nrow(censored_data))) %>%
    purrr::map(~ dplyr::mutate(., os_deceased = os_status == 'DECEASED')) %>%
    purrr::map(~ survival::survfit(Surv(os_months, os_deceased) ~ 1, data = .)) %>%
    purrr::map(fortify)
```

We should now have 2000 estimates of the survival curve, one for each post-warmup draw.

```{r}
class(models)
length(models)
head(models[[1]])
```

How to summarize the estimated uncertainty in our survival curve?

(A: For each observed time, we want to summarize the distribution of `surv` estimates)

Problem is, we don't observe the same timepoints for each iteration. One way to address this would be to aggregate the data for intervals of time.

```{r}
models_agg <- 
    models %>%
    purrr::map(~ dplyr::mutate(., time_group = floor(time))) %>%
    dplyr::bind_rows() %>%
    dplyr::group_by(time_group) %>%
    dplyr::summarize(surv_mean = mean(surv)
                     , surv_p50 = median(surv)
                     , surv_lower = quantile(surv, probs = 0.025)
                     , surv_upper = quantile(surv, probs = 0.975)
                     ) %>%
    dplyr::ungroup()

ggplot(models_agg, aes(x = time_group)) + geom_line(aes(y = surv_mean)) + geom_ribbon(aes(ymin = surv_lower, ymax = surv_upper), alpha = 0.2) + xlim(c(0, max(clin_data$os_months)))
```


Comparing posterior predicted intervals with observed data 

```{r}

ggplot(data = 
           models_agg %>% 
           dplyr::mutate(src = 'posterior predicted values (sampled)') %>% 
           dplyr::rename(time = time_group, surv = surv_mean, lower = surv_lower, upper = surv_upper) %>%
           bind_rows(fortify(mle.surv) %>% mutate(src = 'actual data')),
       aes(x = time, group = src)) + 
    geom_line(aes(y = surv, colour = src)) + 
    geom_ribbon(aes(ymin = lower, ymax = upper, fill = src), alpha = 0.2) +
    xlim(c(0, max(clin_data$os_months))) +
    ggtitle('Survival curve for observed and posterior predicted event times')


```
    

Comparing actual & approximate posterior estimates 

```{r}
ggplot(data = 
           models_agg %>% 
           dplyr::mutate(src = 'posterior predicted values (sampled)') %>% 
           dplyr::rename(time = time_group, surv = surv_mean, lower = surv_lower, upper = surv_upper) %>%
           bind_rows(fortify(mle.surv) %>% mutate(src = 'actual data')) %>%
           bind_rows(fortify(simppd.surv) %>% mutate(src = 'posterior predicted values (using mean)')),
       aes(x = time, group = src)) + 
    geom_line(aes(y = surv, colour = src)) + 
    geom_ribbon(aes(ymin = lower, ymax = upper, fill = src), alpha = 0.2) +
    xlim(c(0, max(clin_data$os_months))) +
    ggtitle('Survival curve for observed and posterior predicted event times')
```


What if we computed this a different way?  

IE pooling together all posterior predicted event times into one big data set.

Would this be a more correct summary than the above? 

```{r}
ppd_data <- 
    purrr::map2(
        .x = pp_alpha,
        .y = pp_mu,
        .f = ~ sim_post_predict(alpha = .x,
                                mu = .y,
                                Nobs = nrow(observed_data),
                                Ncen = nrow(censored_data)
                                )
        ) %>%
    purrr::map(~ dplyr::mutate(., os_deceased = os_status == 'DECEASED')) %>%
    dplyr::bind_rows()

simppd2.surv <- survival::survfit(Surv(os_months, os_deceased) ~ 1, data = ppd_data)

## not run due to processing constraints: 
## simppd2.data <- fortify(simppd2.surv)
## autoplot(simppd2.surv)
```

